{
 "cells": [
  {
   "cell_type": "raw",
   "id": "46a75d74",
   "metadata": {},
   "source": [
    "The objective of this competition is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n",
    "\n",
    "The dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:\n",
    "\n",
    "D_* = Delinquency variables\n",
    "S_* = Spend variables\n",
    "P_* = Payment variables\n",
    "B_* = Balance variables\n",
    "R_* = Risk variables\n",
    "\n",
    "with the following features being categorical:\n",
    "\n",
    "['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "aaa\n",
    "Your task is to predict, for each customer_ID, the probability of a future payment default (target = 1).\n",
    "\n",
    "Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric.\n",
    "\n",
    "Files\n",
    "train_data.csv - training data with multiple statement dates per customer_ID\n",
    "train_labels.csv - target label for each customer_ID\n",
    "test_data.csv - corresponding test data; your objective is to predict the target label for each customer_ID\n",
    "sample_submission.csv - a sample submission file in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4093a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection \n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "# evaluate sklearn histogram gradient boosting algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import random as rd\n",
    "import timeit\n",
    "import math\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40cdb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\formy\\Downloads\\amex-default-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2727e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(r'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "334f3f6c",
   "metadata": {},
   "source": [
    "def selectRandom(df,total_rows,sample_rows = 1000):\n",
    "    #rows = len(df.iloc[:,1])\n",
    "    rows = 1\n",
    "    random_row = math.floor(rd.random()*total_rows)\n",
    "    Sample = df.iloc[[random_row]]\n",
    "    for i in range(sample_rows-1):\n",
    "        random_row = math.floor(rd.random()*total_rows)\n",
    "        sample_row = df.iloc[[random_row]]\n",
    "        Sample = pd.concat([sample_row,Sample])\n",
    "    return Sample"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cba5d6a",
   "metadata": {},
   "source": [
    "def Add_Models(train_data,test_data):\n",
    "    for i in range(model_count):\n",
    "    # Selecting a sample training set\n",
    "        X = selectRandom(train_data,total_rows=1000000*0.8,sample_rows = 1000)\n",
    "        Data = pd.merge(X,labels)\n",
    "        Y = Data['target']\n",
    "        Data = Data.drop(['customer_ID','S_2'], axis=1)\n",
    "        X = Data.iloc[:,Data.columns != 'target']\n",
    "        X = pd.get_dummies(X,columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "        model = HistGradientBoostingClassifier(max_bins=255, max_iter=100,min_samples_leaf = 100)\n",
    "        model.fit(X,Y)\n",
    "        test_data_encoded = pd.get_dummies(test_data,columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "        test_data_encoded['model_'+str('1')] = model.predict(test_data_encoded[X.columns])\n",
    "        return test_data_encoded"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c04cc260",
   "metadata": {},
   "source": [
    "Data is made available in the local database \"csv_database\". SQLAlchemy and SQLite were used to create and load data in the data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "977c40d2",
   "metadata": {},
   "source": [
    "# Create a connector database\n",
    "csv_database = create_engine('sqlite:///csv_database.db')\n",
    "# Building Database by chunking\n",
    "chunk_size = 10000\n",
    "i,j = 0,0\n",
    "os.chdir(r'C:\\Users\\formy\\Downloads\\amex-default-prediction')\n",
    "indexes = []\n",
    "for df in pd.read_csv('test_data.csv',chunksize = chunk_size, iterator = True):\n",
    "    df = df.rename(columns = {c:c.replace(' ','') for c in df.columns})\n",
    "    df.index += j\n",
    "    df.to_sql('data_use',csv_database,if_exists = 'append')\n",
    "    j = df.index[-1]+1\n",
    "    indexes.append(j)\n",
    "    print('| index: {}'.format(j))\n",
    "    \n",
    "csv_database = create_engine('sqlite:///train_csv_database.db')\n",
    "# Building Database by chunking\n",
    "chunk_size = 10000\n",
    "i,j = 0,0\n",
    "os.chdir(r'C:\\Users\\formy\\Downloads\\amex-default-prediction')\n",
    "indexes = []\n",
    "for df in pd.read_csv('train_data.csv',chunksize = chunk_size, iterator = True):\n",
    "    df = df.rename(columns = {c:c.replace(' ','') for c in df.columns})\n",
    "    df.index += j\n",
    "    df.to_sql('train_data',csv_database,if_exists = 'append')\n",
    "    j = df.index[-1]+1\n",
    "    indexes.append(j)\n",
    "    print('| index: {}'.format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32309849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Records\n",
    "csv_database = sqlite3.connect('csv_database.db')\n",
    "pd.read_sql_query('SELECT customer_ID FROM data_use limit 1 ',csv_database)\n",
    "df = pd.read_sql_query('SELECT * FROM data_use where customer_ID = \"00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7\" ',csv_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7408601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9990 entries, 0001337ded4e1c2539d1a78ff44a457bd4a95caa55ba1730b2849b92ea687f9e to 059efd19407f9c2c43c58c65ed65be58ba7144d873846e8effd3dc55b2656dfa\n",
      "Columns: 792 entries, P_2_mean to D_68_6.0_trend\n",
      "dtypes: float64(792)\n",
      "memory usage: 60.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_ready.info(verbose = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a0c4131",
   "metadata": {},
   "source": [
    "Customer ID's are stored seperately for reusing them during our data slicing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26c30949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading customer id's\n",
    "import pickle\n",
    "\n",
    "with open(\"train\", \"rb\") as fp:   #UnPickling train\n",
    "     train_customers = pickle.load(fp)\n",
    "        \n",
    "with open(\"test\", \"rb\") as fp:   #UnPickling test\n",
    "     test_customers = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ea97037",
   "metadata": {},
   "source": [
    "After feauture engineering data is loaded into the database for easy use for later.\n",
    "More feautures can be later added. We can SQL functionality to join to the existing feautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f04ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDataprep(step = 10000):\n",
    "    j = 0\n",
    "    while j<len(train_customers)-1:\n",
    "        last_index = min(j+step,len(test_customers)-1)\n",
    "        customer_slice = train_customers[j:last_index]\n",
    "        j = last_index\n",
    "        slice_query = 'SELECT * FROM raw_train_data WHERE customer_ID in ('+\"'\" + \"','\".join(customer_slice)+\"')\"\n",
    "        df = pd.read_sql_query(slice_query,csv_database)\n",
    "        df_dum = pd.get_dummies(df,columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean') #, weights='uniform', metric='nan_euclidean' \n",
    "        df_d = df_dum.dropna(axis='columns', thresh=0.4*len(df_dum.index))\n",
    "        df_customers = df_d[['customer_ID']]\n",
    "        df_d = df_d.drop(['customer_ID','S_2'], axis=1)\n",
    "        imputer.fit(df_d)\n",
    "        df_imp = pd.DataFrame(imputer.transform(df_d),columns = df_d.columns)\n",
    "        df_imp['index1'] = df_imp.index\n",
    "        df_customers['index1'] = df_customers.index\n",
    "        df_final = df_imp.merge(df_customers)\n",
    "        df_final = df_final.drop(['index','index1'], axis=1)\n",
    "        #df_imp.index = df.index\n",
    "        df_mean = df_final.groupby('customer_ID').agg(mean)\n",
    "        df_var  = df_final.groupby('customer_ID').agg(stdev)\n",
    "        df_range = df_final.groupby('customer_ID').agg(ranges)\n",
    "        df_trend  = df_final.groupby('customer_ID').agg(trend)\n",
    "        df_ready = df_mean.merge(df_var,on = ['customer_ID'],suffixes=('_mean', '_var')).merge(df_range,on = ['customer_ID'],suffixes=('', '_range')).merge(df_trend,on = ['customer_ID'],suffixes=('', '_trend'))\n",
    "        df_ready.to_sql('train_data_refined',csv_database,if_exists = 'append')\n",
    "        k = df_ready.index[-1]+1\n",
    "        print('| index: {}'.format(k))\n",
    "        \n",
    "def testDataprep(step = 10000):\n",
    "    j = 0\n",
    "    while j<len(test_customers)-1:\n",
    "        last_index = min(j+step,len(test_customers)-1)\n",
    "        customer_slice = test_customers[j:last_index]\n",
    "        j = last_index\n",
    "        slice_query = 'SELECT * FROM data_use WHERE customer_ID in ('+\"'\" + \"','\".join(customer_slice)+\"')\"\n",
    "        df = pd.read_sql_query(slice_query,csv_database)\n",
    "        df_dum = pd.get_dummies(df,columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean') #, weights='uniform', metric='nan_euclidean' \n",
    "        df_d = df_dum.dropna(axis='columns', thresh=0.4*len(df_dum.index))\n",
    "        df_customers = df_d[['customer_ID']]\n",
    "        df_d = df_d.drop(['customer_ID','S_2'], axis=1)\n",
    "        imputer.fit(df_d)\n",
    "        df_imp = pd.DataFrame(imputer.transform(df_d),columns = df_d.columns)\n",
    "        df_imp['index1'] = df_imp.index\n",
    "        \n",
    "        df_customers['index1'] = df_customers.index\n",
    "        df_final = df_imp.merge(df_customers)\n",
    "        df_final = df_final.drop(['index','index1'], axis=1)\n",
    "        #df_imp.index = df.index\n",
    "        df_mean = df_final.groupby('customer_ID').agg(mean)\n",
    "        df_var  = df_final.groupby('customer_ID').agg(stdev)\n",
    "        df_range = df_final.groupby('customer_ID').agg(ranges)\n",
    "        df_trend  = df_final.groupby('customer_ID').agg(trend)\n",
    "        df_ready = df_mean.merge(df_var,on = ['customer_ID'],suffixes=('_mean', '_var')).merge(df_range,on = ['customer_ID'],suffixes=('', '_range')).merge(df_trend,on = ['customer_ID'],suffixes=('', '_trend'))\n",
    "        df_ready.to_sql('test_data_refined',csv_database,if_exists = 'append')\n",
    "        k = df_ready.index[-1]+1\n",
    "        print('| index: {}'.format(k))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99dcc03e",
   "metadata": {},
   "source": [
    "Below code was used to load customer_ID's and feautures. Its converted to text, so we dont run it accidently"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df272f85",
   "metadata": {},
   "source": [
    "def stdev(x):\n",
    "    mean = sum(x)/len(x)\n",
    "    return math.sqrt(sum([(i-mean)**2 for i in x]))\n",
    "def mean(x):\n",
    "    return sum(x)/len(x)\n",
    "\n",
    "def ranges(x):\n",
    "    return max(x)-min(x)\n",
    "\n",
    "def trend(x):\n",
    "        return x[max(x.index)]-x[min(x.index)]\n",
    "trainDataprep()\n",
    "testDataprep()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71fe8617",
   "metadata": {},
   "source": [
    "# No need to load again, just load pickle files\n",
    "\n",
    "train_csv_database = sqlite3.connect('train_csv_database.db')\n",
    "train_customers = pd.read_sql_query('SELECT DISTINCT customer_ID FROM data_use',train_csv_database)['customer_ID'].tolist()\n",
    "csv_database = sqlite3.connect('csv_database.db')\n",
    "test_customers = pd.read_sql_query('SELECT DISTINCT customer_ID FROM data_use',csv_database)['customer_ID'].tolist()\n",
    "\n",
    "\n",
    "with open(\"train\", \"wb\") as fp:   #Pickling train\n",
    "     pickle.dump(train_customers, fp)\n",
    "     \n",
    "with open(\"test\", \"wb\") as fp:   #Pickling test\n",
    "     pickle.dump(test_customers, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "868b9be1",
   "metadata": {},
   "source": [
    "Overhead_time = 45 sec\n",
    "scale_time = 4/80 = 0.05 sec\n",
    "Expected_time for 10^6 records = 50000sec = 14 hrs, but it is a one time creation, so store it in a database or atleast csv file for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71bedfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final = pd.read_sql_query('select * from train_data_refined',csv_database)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f1a3bbb",
   "metadata": {},
   "source": [
    "joining with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61a560bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv('train_labels.csv')\n",
    "df_train_with_labels = df_train_final.merge(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a2d756",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train_with_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21896/3772790911.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train_with_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train_with_labels' is not defined"
     ]
    }
   ],
   "source": [
    "df_train_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b90e371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the XGB model on the entire data \n",
    "df_train_with_labels = df_train_with_labels[variable_list_final]\n",
    "X_train = df_train_with_labels.iloc[:,df_train_with_labels.columns != 'target']\n",
    "y_train = df_train_with_labels[['target']]\n",
    "model = XGBClassifier(objective = amex_metric)\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60029110",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0866432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\formy\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1250, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 81, in _train_internal\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 1685, in update\n",
      "    grad, hess = fobj(pred, dtrain)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 89, in inner\n",
      "    return func(labels, preds)\n",
      "TypeError: amex_metric() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\formy\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1250, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 81, in _train_internal\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 1685, in update\n",
      "    grad, hess = fobj(pred, dtrain)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 89, in inner\n",
      "    return func(labels, preds)\n",
      "TypeError: amex_metric() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\formy\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1250, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 81, in _train_internal\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 1685, in update\n",
      "    grad, hess = fobj(pred, dtrain)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 89, in inner\n",
      "    return func(labels, preds)\n",
      "TypeError: amex_metric() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\formy\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1250, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 81, in _train_internal\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 1685, in update\n",
      "    grad, hess = fobj(pred, dtrain)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 89, in inner\n",
      "    return func(labels, preds)\n",
      "TypeError: amex_metric() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\formy\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1250, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 81, in _train_internal\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 1685, in update\n",
      "    grad, hess = fobj(pred, dtrain)\n",
      "  File \"C:\\Users\\formy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 89, in inner\n",
      "    return func(labels, preds)\n",
      "TypeError: amex_metric() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(model, X_train, y_train['target'].tolist(), cv=5,scoring = amex_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3228d3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56530739, 0.56286261, 0.56372263, 0.56587831, 0.56580695])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b907bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_slice = test_customers[0:10]\n",
    "slice_query = 'SELECT * FROM test_data_refined WHERE customer_ID in ('+\"'\" + \"','\".join(customer_slice)+\"')\"\n",
    "test_data_slice = pd.read_sql_query(slice_query,csv_database)\n",
    "variable_list = list(set(test_data_slice.columns) & set(df_train_final.columns))\n",
    "variable_list.append('target')\n",
    "variable_list_final = [i for i in variable_list if i != 'customer_ID']\n",
    "variables = [i for i in variable_list_final if i != 'target']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da294fcf",
   "metadata": {},
   "source": [
    "# loading customer id's\n",
    "\n",
    "df_train_with_labels.to_pickle(\"df_train_with_labels.pkl\")  \n",
    "filename = 'XGBmodel'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "203a8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "filename = 'XGBmodel'\n",
    "XGB_model = pickle.load(open(filename, 'rb'))\n",
    "df_train_with_labels = df_train_with_labels[variable_list_final]\n",
    "X_train = df_train_with_labels.iloc[:,df_train_with_labels.columns != 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68e39cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_train)\n",
    "Y_act = df_train_with_labels['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48e01784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[322594,  17491],\n",
       "       [ 18862,  99966]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(Y_act , Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9621dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(model,X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "310c8ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6533488200028399"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amex_metric(Y_act, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "136e2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Submissions(step = 100000):\n",
    "    Submissions = pd.DataFrame(columns= ['customer_ID','prediction'])\n",
    "    j = 0\n",
    "    stepNum = 0\n",
    "    while j<=len(test_customers):\n",
    "        stepNum+=1\n",
    "        last_index = min(j+step,len(test_customers))\n",
    "        if last_index == j:\n",
    "            break\n",
    "        customer_slice = test_customers[j:last_index]\n",
    "        j = last_index\n",
    "        slice_query = 'SELECT * FROM test_data_refined WHERE customer_ID in ('+\"'\" + \"','\".join(customer_slice)+\"')\"\n",
    "        test_data_slice = pd.read_sql_query(slice_query,csv_database)\n",
    "        X_test = test_data_slice[variables]\n",
    "        y_pred = model.predict(X_test)\n",
    "        Submission_slice = pd.DataFrame()\n",
    "        Submission_slice['customer_ID'] = test_data_slice['customer_ID']\n",
    "        Submission_slice['prediction'] = y_pred\n",
    "        Submissions = pd.concat([Submissions,Submission_slice])\n",
    "        print(str(stepNum))\n",
    "    Submissions.to_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e09f5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "Submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "65f2aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "Submissions = pd.DataFrame(columns= ['customer_ID','predictions'])\n",
    "step = 1000\n",
    "last_index = min(j+step,len(test_customers))\n",
    "customer_slice = test_customers[j:last_index]\n",
    "j = last_index\n",
    "slice_query = 'SELECT * FROM test_data_refined WHERE customer_ID in ('+\"'\" + \"','\".join(customer_slice)+\"')\"\n",
    "test_data_slice = pd.read_sql_query(slice_query,csv_database)\n",
    "X_test = test_data_slice[variables]\n",
    "y_pred = XGB_model.predict(X_test)\n",
    "Submission_slice = pd.DataFrame()\n",
    "Submission_slice['customer_ID'] = test_data_slice['customer_ID']\n",
    "Submission_slice['predictions'] = y_pred\n",
    "Submissions = pd.concat([Submissions,Submission_slice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8a3a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "Submissions = pd.read_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8067efb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924615</th>\n",
       "      <td>24615</td>\n",
       "      <td>ffff824b313399b00db6bc930b83f1a2188d8b1dbd3a31...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924616</th>\n",
       "      <td>24616</td>\n",
       "      <td>ffff952c631f2c911b8a2a8ca56ea6e656309a83d2f64c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924617</th>\n",
       "      <td>24617</td>\n",
       "      <td>ffffcf5df59e5e0bba2a5ac4578a34e2b5aa64a1546cd3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924618</th>\n",
       "      <td>24618</td>\n",
       "      <td>ffffd61f098cc056dbd7d2a21380c4804bbfe60856f475...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924619</th>\n",
       "      <td>24619</td>\n",
       "      <td>ffffddef1fc3643ea179c93245b68dca0f36941cd83977...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>924620 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                        customer_ID  \\\n",
       "0                0  00000469ba478561f23a92a868bd366de6f6527a684c9a...   \n",
       "1                1  00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...   \n",
       "2                2  0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...   \n",
       "3                3  00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...   \n",
       "4                4  00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...   \n",
       "...            ...                                                ...   \n",
       "924615       24615  ffff824b313399b00db6bc930b83f1a2188d8b1dbd3a31...   \n",
       "924616       24616  ffff952c631f2c911b8a2a8ca56ea6e656309a83d2f64c...   \n",
       "924617       24617  ffffcf5df59e5e0bba2a5ac4578a34e2b5aa64a1546cd3...   \n",
       "924618       24618  ffffd61f098cc056dbd7d2a21380c4804bbfe60856f475...   \n",
       "924619       24619  ffffddef1fc3643ea179c93245b68dca0f36941cd83977...   \n",
       "\n",
       "        prediction  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                1  \n",
       "...            ...  \n",
       "924615           0  \n",
       "924616           0  \n",
       "924617           1  \n",
       "924618           0  \n",
       "924619           0  \n",
       "\n",
       "[924620 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06240d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission = pd.DataFrame()\n",
    "Submission['customer_ID'] = Submissions['customer_ID']\n",
    "Submission['prediction'] = Submissions['prediction']\n",
    "Submission.to_csv('Submission1.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24bf9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Submission['prediction'])/len(Submission['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6e577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
