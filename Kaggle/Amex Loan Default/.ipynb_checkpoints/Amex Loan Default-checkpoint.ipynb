{
 "cells": [
  {
   "cell_type": "raw",
   "id": "46a75d74",
   "metadata": {},
   "source": [
    "The objective of this competition is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n",
    "\n",
    "The dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:\n",
    "\n",
    "D_* = Delinquency variables\n",
    "S_* = Spend variables\n",
    "P_* = Payment variables\n",
    "B_* = Balance variables\n",
    "R_* = Risk variables\n",
    "\n",
    "with the following features being categorical:\n",
    "\n",
    "['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "aaa\n",
    "Your task is to predict, for each customer_ID, the probability of a future payment default (target = 1).\n",
    "\n",
    "Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric.\n",
    "\n",
    "Files\n",
    "train_data.csv - training data with multiple statement dates per customer_ID\n",
    "train_labels.csv - target label for each customer_ID\n",
    "test_data.csv - corresponding test data; your objective is to predict the target label for each customer_ID\n",
    "sample_submission.csv - a sample submission file in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4093a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection \n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "# evaluate sklearn histogram gradient boosting algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import random as rd\n",
    "import timeit\n",
    "import math\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061af64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\formy\\Downloads\\amex-default-prediction')\n",
    "Full_data = pd.read_csv(\"train_data.csv\", nrows=1000000)\n",
    "#Full_data = Full_data.drop(columns=['customer_ID', 'S_2'])\n",
    "labels = pd.read_csv(\"train_labels.csv\")\n",
    "train_data,test_data = sklearn.model_selection.train_test_split(Full_data,train_size = 0.8)\n",
    "test_data = pd.merge(test_data,labels)\n",
    "#X = Full_data.sample(frac=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2727e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(r'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "334f3f6c",
   "metadata": {},
   "source": [
    "def selectRandom(df,total_rows,sample_rows = 1000):\n",
    "    #rows = len(df.iloc[:,1])\n",
    "    rows = 1\n",
    "    random_row = math.floor(rd.random()*total_rows)\n",
    "    Sample = df.iloc[[random_row]]\n",
    "    for i in range(sample_rows-1):\n",
    "        random_row = math.floor(rd.random()*total_rows)\n",
    "        sample_row = df.iloc[[random_row]]\n",
    "        Sample = pd.concat([sample_row,Sample])\n",
    "    return Sample"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cba5d6a",
   "metadata": {},
   "source": [
    "def Add_Models(train_data,test_data):\n",
    "    for i in range(model_count):\n",
    "    # Selecting a sample training set\n",
    "        X = selectRandom(train_data,total_rows=1000000*0.8,sample_rows = 1000)\n",
    "        Data = pd.merge(X,labels)\n",
    "        Y = Data['target']\n",
    "        Data = Data.drop(['customer_ID','S_2'], axis=1)\n",
    "        X = Data.iloc[:,Data.columns != 'target']\n",
    "        X = pd.get_dummies(X,columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "        model = HistGradientBoostingClassifier(max_bins=255, max_iter=100,min_samples_leaf = 100)\n",
    "        model.fit(X,Y)\n",
    "        test_data_encoded = pd.get_dummies(test_data,columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "        test_data_encoded['model_'+str('1')] = model.predict(test_data_encoded[X.columns])\n",
    "        return test_data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be5be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data Iteratively\n",
    "DIR = r'C:\\Users\\formy\\Downloads\\amex-default-prediction'\n",
    "FILE = r'\\test_data.csv'\n",
    "file = '{}{}'.format(DIR,FILE)\n",
    "##def Load(test_data):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "977c40d2",
   "metadata": {},
   "source": [
    "# Create a connector database\n",
    "csv_database = create_engine('sqlite:///csv_database.db')\n",
    "# Building Database by chunking\n",
    "chunk_size = 10000\n",
    "i,j = 0,0\n",
    "os.chdir(r'C:\\Users\\formy\\Downloads\\amex-default-prediction')\n",
    "indexes = []\n",
    "for df in pd.read_csv('test_data.csv',chunksize = chunk_size, iterator = True):\n",
    "    df = df.rename(columns = {c:c.replace(' ','') for c in df.columns})\n",
    "    df.index += j\n",
    "    df.to_sql('data_use',csv_database,if_exists = 'append')\n",
    "    j = df.index[-1]+1\n",
    "    indexes.append(j)\n",
    "    print('| index: {}'.format(j))\n",
    "    \n",
    "csv_database = create_engine('sqlite:///train_csv_database.db')\n",
    "# Building Database by chunking\n",
    "chunk_size = 10000\n",
    "i,j = 0,0\n",
    "os.chdir(r'C:\\Users\\formy\\Downloads\\amex-default-prediction')\n",
    "indexes = []\n",
    "for df in pd.read_csv('train_data.csv',chunksize = chunk_size, iterator = True):\n",
    "    df = df.rename(columns = {c:c.replace(' ','') for c in df.columns})\n",
    "    df.index += j\n",
    "    df.to_sql('train_data',csv_database,if_exists = 'append')\n",
    "    j = df.index[-1]+1\n",
    "    indexes.append(j)\n",
    "    print('| index: {}'.format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32309849",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_database = sqlite3.connect('csv_database.db')\n",
    "pd.read_sql_query('SELECT customer_ID FROM data_use limit 1 ',csv_database)\n",
    "df = pd.read_sql_query('SELECT * FROM data_use where customer_ID = \"00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7\" ',csv_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7408601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9990 entries, 0001337ded4e1c2539d1a78ff44a457bd4a95caa55ba1730b2849b92ea687f9e to 059efd19407f9c2c43c58c65ed65be58ba7144d873846e8effd3dc55b2656dfa\n",
      "Columns: 792 entries, P_2_mean to D_68_6.0_trend\n",
      "dtypes: float64(792)\n",
      "memory usage: 60.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_ready.info(verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c30949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading customer id's\n",
    "import pickle\n",
    "\n",
    "with open(\"train\", \"rb\") as fp:   #UnPickling train\n",
    "     train_customers = pickle.load(fp)\n",
    "        \n",
    "with open(\"test\", \"rb\") as fp:   #UnPickling test\n",
    "     test_customers = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f04ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDataprep(step = 10000):\n",
    "    j = 0\n",
    "    while j<len(train_customers)-1:\n",
    "        last_index = min(j+step,len(test_customers)-1)\n",
    "        customer_slice = train_customers[j:last_index]\n",
    "        j = last_index\n",
    "        slice_query = 'SELECT * FROM raw_train_data WHERE customer_ID in ('+\"'\" + \"','\".join(customer_slice)+\"')\"\n",
    "        df = pd.read_sql_query(slice_query,csv_database)\n",
    "        df_dum = pd.get_dummies(df,columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean') #, weights='uniform', metric='nan_euclidean' \n",
    "        df_d = df_dum.dropna(axis='columns', thresh=0.4*len(df_dum.index))\n",
    "        df_customers = df_d[['customer_ID']]\n",
    "        df_d = df_d.drop(['customer_ID','S_2'], axis=1)\n",
    "        imputer.fit(df_d)\n",
    "        df_imp = pd.DataFrame(imputer.transform(df_d),columns = df_d.columns)\n",
    "        df_imp['index1'] = df_imp.index\n",
    "        df_customers['index1'] = df_customers.index\n",
    "        df_final = df_imp.merge(df_customers)\n",
    "        df_final = df_final.drop(['index','index1'], axis=1)\n",
    "        #df_imp.index = df.index\n",
    "        df_mean = df_final.groupby('customer_ID').agg(mean)\n",
    "        df_var  = df_final.groupby('customer_ID').agg(stdev)\n",
    "        df_range = df_final.groupby('customer_ID').agg(ranges)\n",
    "        df_trend  = df_final.groupby('customer_ID').agg(trend)\n",
    "        df_ready = df_mean.merge(df_var,on = ['customer_ID'],suffixes=('_mean', '_var')).merge(df_range,on = ['customer_ID'],suffixes=('', '_range')).merge(df_trend,on = ['customer_ID'],suffixes=('', '_trend'))\n",
    "        df_ready.to_sql('train_data_refined',csv_database,if_exists = 'append')\n",
    "        k = df_ready.index[-1]+1\n",
    "        print('| index: {}'.format(k))\n",
    "        \n",
    "def testDataprep(step = 10000):\n",
    "    j = 0\n",
    "    while j<len(test_customers)-1:\n",
    "        last_index = min(j+step,len(test_customers)-1)\n",
    "        customer_slice = test_customers[j:last_index]\n",
    "        j = last_index\n",
    "        slice_query = 'SELECT * FROM data_use WHERE customer_ID in ('+\"'\" + \"','\".join(customer_slice)+\"')\"\n",
    "        df = pd.read_sql_query(slice_query,csv_database)\n",
    "        df_dum = pd.get_dummies(df,columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean') #, weights='uniform', metric='nan_euclidean' \n",
    "        df_d = df_dum.dropna(axis='columns', thresh=0.4*len(df_dum.index))\n",
    "        df_customers = df_d[['customer_ID']]\n",
    "        df_d = df_d.drop(['customer_ID','S_2'], axis=1)\n",
    "        imputer.fit(df_d)\n",
    "        df_imp = pd.DataFrame(imputer.transform(df_d),columns = df_d.columns)\n",
    "        df_imp['index1'] = df_imp.index\n",
    "        \n",
    "        df_customers['index1'] = df_customers.index\n",
    "        df_final = df_imp.merge(df_customers)\n",
    "        df_final = df_final.drop(['index','index1'], axis=1)\n",
    "        #df_imp.index = df.index\n",
    "        df_mean = df_final.groupby('customer_ID').agg(mean)\n",
    "        df_var  = df_final.groupby('customer_ID').agg(stdev)\n",
    "        df_range = df_final.groupby('customer_ID').agg(ranges)\n",
    "        df_trend  = df_final.groupby('customer_ID').agg(trend)\n",
    "        df_ready = df_mean.merge(df_var,on = ['customer_ID'],suffixes=('_mean', '_var')).merge(df_range,on = ['customer_ID'],suffixes=('', '_range')).merge(df_trend,on = ['customer_ID'],suffixes=('', '_trend'))\n",
    "        df_ready.to_sql('test_data_refined',csv_database,if_exists = 'append')\n",
    "        k = df_ready.index[-1]+1\n",
    "        print('| index: {}'.format(k))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df272f85",
   "metadata": {},
   "source": [
    "def stdev(x):\n",
    "    mean = sum(x)/len(x)\n",
    "    return math.sqrt(sum([(i-mean)**2 for i in x]))\n",
    "def mean(x):\n",
    "    return sum(x)/len(x)\n",
    "\n",
    "def ranges(x):\n",
    "    return max(x)-min(x)\n",
    "\n",
    "def trend(x):\n",
    "        return x[max(x.index)]-x[min(x.index)]\n",
    "trainDataprep()\n",
    "testDataprep()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71fe8617",
   "metadata": {},
   "source": [
    "# No need to load again, just load pickle files\n",
    "\n",
    "train_csv_database = sqlite3.connect('train_csv_database.db')\n",
    "train_customers = pd.read_sql_query('SELECT DISTINCT customer_ID FROM data_use',train_csv_database)['customer_ID'].tolist()\n",
    "csv_database = sqlite3.connect('csv_database.db')\n",
    "test_customers = pd.read_sql_query('SELECT DISTINCT customer_ID FROM data_use',csv_database)['customer_ID'].tolist()\n",
    "\n",
    "\n",
    "with open(\"train\", \"wb\") as fp:   #Pickling train\n",
    "     pickle.dump(train_customers, fp)\n",
    "     \n",
    "with open(\"test\", \"wb\") as fp:   #Pickling test\n",
    "     pickle.dump(test_customers, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "868b9be1",
   "metadata": {},
   "source": [
    "Overhead_time = 45 sec\n",
    "scale_time = 4/80 = 0.05 sec\n",
    "Expected_time for 10^6 records = 50000sec = 14 hrs, but it is a one time creation, so store it in a database or atleast csv file for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71bedfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final = pd.read_sql_query('select * from train_data_refined',csv_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2a2d756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>P_2_mean</th>\n",
       "      <th>D_39_mean</th>\n",
       "      <th>B_1_mean</th>\n",
       "      <th>B_2_mean</th>\n",
       "      <th>R_1_mean</th>\n",
       "      <th>S_3_mean</th>\n",
       "      <th>D_41_mean</th>\n",
       "      <th>B_3_mean</th>\n",
       "      <th>D_43_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>D_66_0.0_trend</th>\n",
       "      <th>D_66_1.0_trend</th>\n",
       "      <th>D_68_0.0_trend</th>\n",
       "      <th>D_68_1.0_trend</th>\n",
       "      <th>D_68_2.0_trend</th>\n",
       "      <th>D_68_3.0_trend</th>\n",
       "      <th>D_68_4.0_trend</th>\n",
       "      <th>D_68_5.0_trend</th>\n",
       "      <th>D_68_6.0_trend</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0.933824</td>\n",
       "      <td>0.010704</td>\n",
       "      <td>0.012007</td>\n",
       "      <td>1.005086</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.113215</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>0.006456</td>\n",
       "      <td>0.155422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n",
       "      <td>0.899820</td>\n",
       "      <td>0.215205</td>\n",
       "      <td>0.025654</td>\n",
       "      <td>0.991083</td>\n",
       "      <td>0.006246</td>\n",
       "      <td>0.120578</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.005663</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n",
       "      <td>0.878454</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.815677</td>\n",
       "      <td>0.006621</td>\n",
       "      <td>0.227293</td>\n",
       "      <td>0.006842</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.155422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n",
       "      <td>0.598969</td>\n",
       "      <td>0.048862</td>\n",
       "      <td>0.059876</td>\n",
       "      <td>0.955264</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.247750</td>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.006423</td>\n",
       "      <td>0.061026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n",
       "      <td>0.891679</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.814543</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.210619</td>\n",
       "      <td>0.005352</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.048778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458908</th>\n",
       "      <td>ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...</td>\n",
       "      <td>0.848750</td>\n",
       "      <td>0.119236</td>\n",
       "      <td>0.029180</td>\n",
       "      <td>1.005166</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>0.125981</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.153978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458909</th>\n",
       "      <td>ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...</td>\n",
       "      <td>0.859327</td>\n",
       "      <td>0.066421</td>\n",
       "      <td>0.368335</td>\n",
       "      <td>0.042358</td>\n",
       "      <td>0.005542</td>\n",
       "      <td>0.250686</td>\n",
       "      <td>0.005306</td>\n",
       "      <td>0.271978</td>\n",
       "      <td>0.182071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458910</th>\n",
       "      <td>ffff9984b999fccb2b6127635ed0736dda94e544e67e02...</td>\n",
       "      <td>0.786838</td>\n",
       "      <td>0.221548</td>\n",
       "      <td>0.043031</td>\n",
       "      <td>0.854813</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.084385</td>\n",
       "      <td>0.182801</td>\n",
       "      <td>0.057544</td>\n",
       "      <td>0.121265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458911</th>\n",
       "      <td>ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...</td>\n",
       "      <td>0.804454</td>\n",
       "      <td>0.030878</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.675695</td>\n",
       "      <td>0.023470</td>\n",
       "      <td>0.357946</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>0.071122</td>\n",
       "      <td>0.104824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458912</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>0.983617</td>\n",
       "      <td>0.290399</td>\n",
       "      <td>0.037961</td>\n",
       "      <td>0.661809</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.055203</td>\n",
       "      <td>0.004240</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458913 rows × 794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_ID  P_2_mean  \\\n",
       "0       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  0.933824   \n",
       "1       00000fd6641609c6ece5454664794f0340ad84dddce9a2...  0.899820   \n",
       "2       00001b22f846c82c51f6e3958ccd81970162bae8b007e8...  0.878454   \n",
       "3       000041bdba6ecadd89a52d11886e8eaaec9325906c9723...  0.598969   \n",
       "4       00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...  0.891679   \n",
       "...                                                   ...       ...   \n",
       "458908  ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...  0.848750   \n",
       "458909  ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...  0.859327   \n",
       "458910  ffff9984b999fccb2b6127635ed0736dda94e544e67e02...  0.786838   \n",
       "458911  ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...  0.804454   \n",
       "458912  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...  0.983617   \n",
       "\n",
       "        D_39_mean  B_1_mean  B_2_mean  R_1_mean  S_3_mean  D_41_mean  \\\n",
       "0        0.010704  0.012007  1.005086  0.004509  0.113215   0.005021   \n",
       "1        0.215205  0.025654  0.991083  0.006246  0.120578   0.004993   \n",
       "2        0.004181  0.004386  0.815677  0.006621  0.227293   0.006842   \n",
       "3        0.048862  0.059876  0.955264  0.005665  0.247750   0.005490   \n",
       "4        0.004644  0.005941  0.814543  0.004180  0.210619   0.005352   \n",
       "...           ...       ...       ...       ...       ...        ...   \n",
       "458908   0.119236  0.029180  1.005166  0.005458  0.125981   0.003923   \n",
       "458909   0.066421  0.368335  0.042358  0.005542  0.250686   0.005306   \n",
       "458910   0.221548  0.043031  0.854813  0.004751  0.084385   0.182801   \n",
       "458911   0.030878  0.018161  0.675695  0.023470  0.357946   0.004919   \n",
       "458912   0.290399  0.037961  0.661809  0.004153  0.055203   0.004240   \n",
       "\n",
       "        B_3_mean  D_43_mean  ...  D_66_0.0_trend  D_66_1.0_trend  \\\n",
       "0       0.006456   0.155422  ...             0.0             0.0   \n",
       "1       0.005663   0.149579  ...             0.0             0.0   \n",
       "2       0.005493   0.155422  ...             0.0             0.0   \n",
       "3       0.006423   0.061026  ...             0.0             0.0   \n",
       "4       0.005088   0.048778  ...             0.0             0.0   \n",
       "...          ...        ...  ...             ...             ...   \n",
       "458908  0.005814   0.153978  ...             0.0             0.0   \n",
       "458909  0.271978   0.182071  ...             0.0             0.0   \n",
       "458910  0.057544   0.121265  ...             0.0             0.0   \n",
       "458911  0.071122   0.104824  ...             0.0             0.0   \n",
       "458912  0.036802   0.012954  ...             0.0             0.0   \n",
       "\n",
       "        D_68_0.0_trend  D_68_1.0_trend  D_68_2.0_trend  D_68_3.0_trend  \\\n",
       "0                  0.0             0.0             0.0             0.0   \n",
       "1                  0.0             0.0             0.0             0.0   \n",
       "2                  0.0             0.0             0.0             0.0   \n",
       "3                  0.0             0.0            -1.0             1.0   \n",
       "4                  0.0             0.0             0.0             0.0   \n",
       "...                ...             ...             ...             ...   \n",
       "458908             0.0             0.0             0.0             0.0   \n",
       "458909             0.0             0.0             0.0             0.0   \n",
       "458910             0.0             0.0             0.0             0.0   \n",
       "458911             0.0             0.0             0.0             1.0   \n",
       "458912             0.0             0.0             0.0             0.0   \n",
       "\n",
       "        D_68_4.0_trend  D_68_5.0_trend  D_68_6.0_trend  target  \n",
       "0                  0.0             0.0             0.0       0  \n",
       "1                  0.0             0.0             0.0       0  \n",
       "2                  0.0             0.0             0.0       0  \n",
       "3                  0.0             0.0             0.0       0  \n",
       "4                  0.0             0.0             0.0       0  \n",
       "...                ...             ...             ...     ...  \n",
       "458908             0.0             0.0             0.0       0  \n",
       "458909             0.0             0.0             0.0       0  \n",
       "458910             0.0             0.0             0.0       0  \n",
       "458911            -1.0             0.0             0.0       1  \n",
       "458912             0.0             1.0            -1.0       0  \n",
       "\n",
       "[458913 rows x 794 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61a560bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv('train_labels.csv')\n",
    "df_train_with_labels = df_train_final.merge(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "019cf693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:04:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model no training data\n",
    "from xgboost import XGBClassifier\n",
    "df_train_with_labels = df_train_with_labels[variable_list_final]\n",
    "X_train = df_train_with_labels.iloc[:,df_train_with_labels.columns != 'target']\n",
    "y_train = df_train_with_labels[['target']]\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da294fcf",
   "metadata": {},
   "source": [
    "# loading customer id's\n",
    "import pickle\n",
    "df_train_with_labels.to_pickle(\"df_train_with_labels.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "203a8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'XGBmodel'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "# some time later...\n",
    "# load the model from disk\n",
    "XGB_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "136e2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Submissions(step = 100000):\n",
    "    Submissions = pd.DataFrame(columns= ['customer_ID','prediction'])\n",
    "    j = 0\n",
    "    stepNum = 0\n",
    "    while j<=len(test_customers):\n",
    "        stepNum+=1\n",
    "        last_index = min(j+step,len(test_customers))\n",
    "        if last_index == j:\n",
    "            break\n",
    "        customer_slice = test_customers[j:last_index]\n",
    "        j = last_index\n",
    "        slice_query = 'SELECT * FROM test_data_refined WHERE customer_ID in ('+\"'\" + \"','\".join(customer_slice)+\"')\"\n",
    "        test_data_slice = pd.read_sql_query(slice_query,csv_database)\n",
    "        X_test = test_data_slice[variables]\n",
    "        y_pred = XGB_model.predict(X_test)\n",
    "        Submission_slice = pd.DataFrame()\n",
    "        Submission_slice['customer_ID'] = test_data_slice['customer_ID']\n",
    "        Submission_slice['prediction'] = y_pred\n",
    "        Submissions = pd.concat([Submissions,Submission_slice])\n",
    "        print(str(stepNum))\n",
    "    Submissions.to_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e09f5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "Submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b907bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_list = list(set(test_data_slice.columns) & set(df_train_final.columns))\n",
    "variable_list.append('target')\n",
    "variable_list_final = [i for i in variable_list if i != 'customer_ID']\n",
    "variables = [i for i in variable_list_final if i != 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "65f2aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "Submissions = pd.DataFrame(columns= ['customer_ID','predictions'])\n",
    "step = 1000\n",
    "last_index = min(j+step,len(test_customers))\n",
    "customer_slice = test_customers[j:last_index]\n",
    "j = last_index\n",
    "slice_query = 'SELECT * FROM test_data_refined WHERE customer_ID in ('+\"'\" + \"','\".join(customer_slice)+\"')\"\n",
    "test_data_slice = pd.read_sql_query(slice_query,csv_database)\n",
    "X_test = test_data_slice[variables]\n",
    "y_pred = XGB_model.predict(X_test)\n",
    "Submission_slice = pd.DataFrame()\n",
    "Submission_slice['customer_ID'] = test_data_slice['customer_ID']\n",
    "Submission_slice['predictions'] = y_pred\n",
    "Submissions = pd.concat([Submissions,Submission_slice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d8a3a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "Submissions = pd.read_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "06240d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission = pd.DataFrame()\n",
    "Submission['customer_ID'] = Submissions['customer_ID']\n",
    "Submission['prediction'] = Submissions['predictions']\n",
    "Submission.to_csv('Submission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e24bf9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2410666003331098"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Submission['prediction'])/len(Submission['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6e577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
